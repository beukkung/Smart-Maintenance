{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from scipy import signal \n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import os, glob\n",
    "from scipy.signal import butter, lfilter\n",
    "from scipy.signal import freqs\n",
    "from numpy.fft import fft, ifft,rfft,fftfreq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defined custom function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fftx(x):\n",
    "  sr = 51200 #sampling rate\n",
    "  X = abs(rfft(x)) #transform to FFT\n",
    "  n = len(x)\n",
    "  freq = fftfreq(X.size, 1/sr)\n",
    "  X = X[((freq>10) & (freq<200))] #bandpass with 10hz to 200hz\n",
    "  X = X-150 #Simply delete noise with offset value \n",
    "  return X\n",
    "  \n",
    "def bandpass(data1,data2,data3):\n",
    "  A=fftx(data1)\n",
    "  B=fftx(data2)\n",
    "  C=fftx(data3)\n",
    "  return  A,B,C\n",
    "\n",
    "def process_data(data_path):\n",
    "  x = pd.read_csv(data_path,header=None,names=['x','y','z']) \n",
    "  A,B,C = bandpass((x['x']),(x['y']),(x['z']))\n",
    "  x_all.append(A)\n",
    "  y_all.append(B)\n",
    "  z_all.append(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = glob.glob(\"train_data600\\\\train*.csv\")\n",
    "data_path2 = glob.glob(\"train_data1200\\\\train*.csv\")\n",
    "data_path = data_path+data_path2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process Data and save to npy file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_all = []\n",
    "y_all = []\n",
    "z_all = []\n",
    "for path in tqdm(data_path):\n",
    "  process_data(path)\n",
    "  \n",
    "np.save(\"X_all.npy\", x_all)\n",
    "np.save(\"Y_all.npy\", y_all)\n",
    "np.save(\"Z_all.npy\", z_all)\n",
    "np.save(\"Label.npy\", y_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make multi label for multi task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x_all = np.load(\"X_all.npy\")\n",
    "y_all = np.load(\"Y_all.npy\")\n",
    "z_all = np.load(\"Z_all.npy\")\n",
    "y_label = np.load(\"Label.npy\")\n",
    "xyz = np.transpose(np.stack((x_all,y_all,z_all)) , [1,2,0])\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "readme = ['Normal', 'UB_1P', 'UB_2P', 'UB_3P', 'MA_P16', 'MA_P20', 'MA_P46', 'MP_N13', 'MP_N16',\n",
    "          'MP_N18', 'MP_N24', 'MA_P16+UB_1P', 'MA_P16+UB_2P', 'MA_P16+UB_3P', 'MA_P20+UB_1P',\n",
    "          'MA_P20+UB_2P', 'MA_P20+UB_3P', 'MA_P46+UB_1P', 'MA_P46+UB_2P', 'MA_P46+UB_3P', 'MP_N13+UB_1P', 'MP_N13+UB_2P', 'MP_N13+UB_3P', \n",
    "          'MP_N16+UB_1P', 'MP_N16+UB_2P', 'MP_N16+UB_3P', 'MP_N18+UB_1P', 'MP_N18+UB_2P', 'MP_N18+UB_3P', 'Mixcase']\n",
    "UP = ['no', 'UB_1P', 'UB_2P', 'UB_3P']\n",
    "MA = ['no', 'MA_P16', 'MA_P20', 'MA_P46']\n",
    "MP = ['no', 'MP_N13', 'MP_N16','MP_N18', 'MP_N24']\n",
    "fail2index = { w :i for i, w in enumerate(readme)}\n",
    "index2fail = { i :w for i, w in enumerate(readme)}\n",
    "\n",
    "def onehotUP(label):\n",
    "  a = np.zeros(4)\n",
    "  a[label]=1\n",
    "  return a\n",
    "def onehotMA(label):\n",
    "  a = np.zeros(4)\n",
    "  i = (label%4) +1\n",
    "  a[i] = 1\n",
    "  return a\n",
    "def onehotMP(label):\n",
    "  a = np.zeros(5)\n",
    "  i = (label%7) +1\n",
    "  a[i] = 1\n",
    "  return a\n",
    "def makeonehot(fail):\n",
    "  fail = fail.split('+')\n",
    "  yup=[]\n",
    "  yma=[]\n",
    "  ymp=[]\n",
    "  for f in fail:\n",
    "    if f in fail2index:\n",
    "      if fail2index[f]>=1 and fail2index[f]<=3: yup = onehotUP(fail2index[f])\n",
    "      if fail2index[f]>=4 and fail2index[f]<=6: yma = onehotMA(fail2index[f])\n",
    "      if fail2index[f]>=7 and fail2index[f]<=10: ymp = onehotMP(fail2index[f])\n",
    "  if len(yup)==0: yup = np.array([1,0,0,0], dtype=\"float32\")\n",
    "  if len(yma)==0: yma = np.array([1,0,0,0], dtype=\"float32\")\n",
    "  if len(ymp)==0: ymp = np.array([1,0,0,0,0], dtype=\"float32\")\n",
    "  return yup, yma, ymp\n",
    "def decode_up(array): return UP[np.argmax(array)]\n",
    "def decode_ma(array): return MA[np.argmax(array)]\n",
    "def decode_mp(array): return MP[np.argmax(array)]\n",
    "fail = 'Normal'\n",
    "a,b,c = makeonehot(fail)\n",
    "print( a,b,c)\n",
    "print(decode_up(a),decode_ma(b),decode_mp(c))\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split( xyz, y_label, test_size=0.15)\n",
    "\n",
    "\n",
    "yUP_train=[]\n",
    "yMA_train=[]\n",
    "yMP_train=[]\n",
    "yUP_test=[]\n",
    "yMA_test=[]\n",
    "yMP_test=[]\n",
    "for label in y_train:\n",
    "  fail = index2fail[label]\n",
    "  yup, yma, ymp = makeonehot(fail)\n",
    "  yUP_train.append(yup)\n",
    "  yMA_train.append(yma)\n",
    "  yMP_train.append(ymp)\n",
    "  \n",
    "for label in y_test:\n",
    "  fail = index2fail[label]\n",
    "  yup, yma, ymp = makeonehot(fail)\n",
    "  yUP_test.append(yup)\n",
    "  yMA_test.append(yma)\n",
    "  yMP_test.append(ymp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers , models\n",
    "from tensorflow.keras.layers import Flatten, Dense, Conv1D, MaxPool1D, Dropout, BatchNormalization\n",
    "\n",
    "input_shape = (23,3)\n",
    "input_layer = layers.Input(input_shape)\n",
    "\n",
    "conv1D1 = Conv1D(filters=3, kernel_size=(3,), padding='same', activation=tf.keras.layers.LeakyReLU(alpha=0.001))(input_layer)\n",
    "batch1 = BatchNormalization()(conv1D1)\n",
    "\n",
    "conv1D2 = Conv1D(filters=9, kernel_size=(3,), padding='same', activation=tf.keras.layers.LeakyReLU(alpha=0.001))(batch1)\n",
    "batch2 = BatchNormalization()(conv1D2)\n",
    "pool = MaxPool1D(pool_size=(3,), strides=2, padding='same')(batch4)\n",
    "\n",
    "conv1D3 = Conv1D(filters=27, kernel_size=(3,), padding='same', activation=tf.keras.layers.LeakyReLU(alpha=0.001))(batch2)\n",
    "batch3 = BatchNormalization()(conv1D3)\n",
    "\n",
    "conv1D4 = Conv1D(filters=81, kernel_size=(3,), padding='same', activation=tf.keras.layers.LeakyReLU(alpha=0.001))(batch3)\n",
    "batch4 = BatchNormalization()(conv1D4)\n",
    "\n",
    "pool = MaxPool1D(pool_size=(3,), strides=2, padding='same')(batch4)\n",
    "drop = Dropout(0.5)(pool)\n",
    "flat = Flatten()(pool)\n",
    "\n",
    "dense1 = Dense(units = 128, activation=tf.keras.layers.LeakyReLU(alpha=0.001))(flat)\n",
    "dense2 = Dense(units = 128, activation=tf.keras.layers.LeakyReLU(alpha=0.001))(dense1)\n",
    "dense3 = Dense(units = 64, activation=tf.keras.layers.LeakyReLU(alpha=0.001))(dense2)\n",
    "\n",
    "outputUB = Dense(units = len(UP), activation='softmax')(dense3)\n",
    "outputMA = Dense(units = len(MA), activation='softmax')(dense3)\n",
    "outputMP = Dense(units = len(MP), activation='softmax')(dense3)\n",
    "model = models.Model(inputs=input_layer, outputs=[outputUB , outputMA , outputMP])\n",
    "loss = tf.keras.losses.categorical_crossentropy\n",
    "optimizer = tf.keras.optimizers.Adam(lr=1e-5)\n",
    "model.compile(optimizer=optimizer, loss = loss, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import os\n",
    "modelPath = os.path.join(os.getcwd(), 'bestModel.h5')\n",
    "\n",
    "earlystopping = EarlyStopping(\n",
    "    monitor='val_dense_47_loss', # set monitor metrics\n",
    "    min_delta=0.001, # set minimum metrics delta\n",
    "    patience=20, # number of epochs to stop training\n",
    "    restore_best_weights=True, # set if use best weights or last weights\n",
    "    )\n",
    "callbacksList = [earlystopping] # build callbacks list\n",
    "\n",
    "model_history = model.fit( x_train , [np.array(yUP_train), np.array(yMA_train), np.array(yMP_train)], epochs=300, \n",
    "                  batch_size = 64, \n",
    "                validation_data = ( x_test, [np.array(yUP_test), np.array(yMA_test), np.array(yMP_test)] ), \n",
    "                callbacks=callbacksList)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list_test = os.listdir(\"H:\\\\Hackathon2\\\\test\")\n",
    "len(file_list_test)\n",
    "path_dat = \"H:\\\\Hackathon2\\\\test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "data_path = glob.glob(\"H:\\Hackathon2\\\\test\\\\test*.csv\")\n",
    "import numpy as np\n",
    "from numpy.fft import fft, ifft, fftfreq\n",
    "def process_data(data_path):\n",
    "  x = pd.read_csv(data_path,header=None,names=['x','y','z']) \n",
    "  A,B,C = bandpasskub((x['x']),(x['y']),(x['z']))\n",
    "  x_all_test.append(A)\n",
    "  y_all_test.append(B)\n",
    "  z_all_test.append(C)\n",
    "  \n",
    "x_all_test = []\n",
    "y_all_test = []\n",
    "z_all_test = []\n",
    "for path in tqdm(data_path):\n",
    "  process_data(path)\n",
    "  \n",
    "np.save(\"X_alltest.npy\", x_all_test)\n",
    "np.save(\"Y_alltest.npy\", y_all_test)\n",
    "np.save(\"Z_alltest.npy\", z_all_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.load(\"X_alltest.npy\")\n",
    "y_test = np.load(\"Y_alltest.npy\")\n",
    "z_test = np.load(\"Z_alltest.npy\")\n",
    "xyz_test = np.transpose(np.stack((x_test,y_test,z_test)) , [1,2,0])\n",
    "xyz_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(xyz_test)\n",
    "from tqdm import tqdm\n",
    "predict = []\n",
    "for i in tqdm(range(1800)):\n",
    "  a,b,c = pred[0][i], pred[1][i], pred[2][i] \n",
    "  # print(a,b,c)\n",
    "  pr = []\n",
    "  p = [decode_up(a),decode_ma(b),decode_mp(c)]\n",
    "  # print(p)\n",
    "  for j in p:\n",
    "    if j!='no': pr.append(j)\n",
    "  if len(pr)>0 and len(pr)<3:\n",
    "    pr = '+'.join(sorted(pr))\n",
    "  elif len(pr)==3:\n",
    "    pr = 'Mixcase'\n",
    "  else:\n",
    "    pr = 'Normal'\n",
    "  if pr in fail2index: predict.append(fail2index[pr])\n",
    "  else: predict.append(fail2index['Normal'])\n",
    "print(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read Sample and save your submission\n",
    "sample_submission=pd.read_csv('Sample_submission.csv')\n",
    "df = pd.DataFrame({ 'filename':sample_submission['filename'], 'code':predict })\n",
    "df.to_csv('Submitfinal.csv')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
